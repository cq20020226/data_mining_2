import pandas as pd
import json
import os
import time
from glob import glob
import matplotlib
matplotlib.use('Agg')  # å…³é—­å›¾å½¢åç«¯ï¼Œé€‚åˆè¿œç¨‹ç¯å¢ƒ

# è·¯å¾„è®¾ç½®
dataset_dir = '/data/DM_work/dataset/30G_data_new'
product_info_path = os.path.join(dataset_dir, 'product_catalog.json')
output_dir = '/data/DM_work/output/data_preprocess_output'
os.makedirs(output_dir, exist_ok=True)
output_file = os.path.join(output_dir, 'purchase_history_final.jsonl')

# å•†å“ä¿¡æ¯å­—å…¸ï¼ˆid -> {category, price}ï¼‰
with open(product_info_path, 'r', encoding='utf-8') as f:
    product_info = json.load(f)

id_map = {
    prod["id"]: {
        "category": prod["category"],
        "price": prod["price"]
    } for prod in product_info["products"]
}

# å°ç±» -> å¤§ç±» æ˜ å°„è¡¨
category_map = {
    "æ™ºèƒ½æ‰‹æœº": "ç”µå­äº§å“", "ç¬”è®°æœ¬ç”µè„‘": "ç”µå­äº§å“", "å¹³æ¿ç”µè„‘": "ç”µå­äº§å“", "æ™ºèƒ½æ‰‹è¡¨": "ç”µå­äº§å“",
    "è€³æœº": "ç”µå­äº§å“", "éŸ³å“": "ç”µå­äº§å“", "ç›¸æœº": "ç”µå­äº§å“", "æ‘„åƒæœº": "ç”µå­äº§å“", "æ¸¸æˆæœº": "ç”µå­äº§å“",
    "ä¸Šè¡£": "æœè£…", "è£¤å­": "æœè£…", "è£™å­": "æœè£…", "å†…è¡£": "æœè£…", "é‹å­": "æœè£…",
    "å¸½å­": "æœè£…", "æ‰‹å¥—": "æœè£…", "å›´å·¾": "æœè£…", "å¤–å¥—": "æœè£…",
    "é›¶é£Ÿ": "é£Ÿå“", "é¥®æ–™": "é£Ÿå“", "è°ƒå‘³å“": "é£Ÿå“", "ç±³é¢": "é£Ÿå“",
    "æ°´äº§": "é£Ÿå“", "è‚‰ç±»": "é£Ÿå“", "è›‹å¥¶": "é£Ÿå“", "æ°´æœ": "é£Ÿå“", "è”¬èœ": "é£Ÿå“",
    "å®¶å…·": "å®¶å±…", "åºŠä¸Šç”¨å“": "å®¶å±…", "å¨å…·": "å®¶å±…", "å«æµ´ç”¨å“": "å®¶å±…",
    "æ–‡å…·": "åŠå…¬", "åŠå…¬ç”¨å“": "åŠå…¬",
    "å¥èº«å™¨æ": "è¿åŠ¨æˆ·å¤–", "æˆ·å¤–è£…å¤‡": "è¿åŠ¨æˆ·å¤–",
    "ç©å…·": "ç©å…·", "æ¨¡å‹": "ç©å…·", "ç›Šæ™ºç©å…·": "ç©å…·",
    "å©´å„¿ç”¨å“": "æ¯å©´", "å„¿ç«¥è¯¾å¤–è¯»ç‰©": "æ¯å©´",
    "è½¦è½½ç”µå­": "æ±½è½¦ç”¨å“", "æ±½è½¦è£…é¥°": "æ±½è½¦ç”¨å“"
}

# æŸ¥æ‰¾ parquet æ–‡ä»¶
start_time = time.time()
parquet_files = glob(os.path.join(dataset_dir, '*.parquet'))
if not parquet_files:
    print("âŒ æœªæ‰¾åˆ° Parquet æ–‡ä»¶")
    exit()
print(f"ğŸ“¦ å…±æ‰¾åˆ° {len(parquet_files)} ä¸ª Parquet æ–‡ä»¶")

# æ‰“å¼€è¾“å‡ºæµ
record_count = 0
with open(output_file, 'w', encoding='utf-8') as f_out:
    for idx, file in enumerate(parquet_files):
        print(f"\nğŸ“‚ æ­£åœ¨å¤„ç†æ–‡ä»¶ {idx + 1}/{len(parquet_files)}: {file}")
        try:
            df = pd.read_parquet(file, engine='pyarrow')
        except Exception as e:
            print(f"âš ï¸ è¯»å–å¤±è´¥: {e}")
            continue

        if 'purchase_history' not in df.columns:
            print("âš ï¸ è·³è¿‡ï¼šå­—æ®µ 'purchase_history' ä¸å­˜åœ¨")
            continue

        for raw in df['purchase_history']:
            try:
                record = json.loads(raw)
            except json.JSONDecodeError:
                print("âš ï¸ è·³è¿‡æ— æ³•è§£æçš„è®°å½•")
                continue

            # åˆ é™¤æ— æ•ˆå­—æ®µ
            record.pop("categories", None)
            record.pop("avg_price", None)

            # è¡¥å…… items ä¿¡æ¯
            new_items = []
            for item in record.get("items", []):
                prod_id = item.get("id")
                if prod_id in id_map:
                    category = id_map[prod_id]["category"]
                    new_items.append({
                        "id": prod_id,
                        "category": category,
                        "price": id_map[prod_id]["price"],
                        "major_category": category_map.get(category, "å…¶ä»–")
                    })
                else:
                    print(f"âš ï¸ å•†å“ID {prod_id} æœªæ‰¾åˆ°ï¼Œè·³è¿‡")

            # æ›´æ–°è®°å½•
            record["items"] = new_items
            record["item_count"] = len(new_items)

            # å†™å…¥ jsonl
            f_out.write(json.dumps(record, ensure_ascii=False) + '\n')
            record_count += 1
            if record_count % 100000 == 0:
                print(f"âœ… å·²å†™å…¥è®°å½•: {record_count:,}")

elapsed = time.time() - start_time
print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼Œè®°å½•æ•°ï¼š{record_count:,}")
print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_file}")
print(f"â±ï¸ æ€»ç”¨æ—¶ï¼š{elapsed:.2f} ç§’")
